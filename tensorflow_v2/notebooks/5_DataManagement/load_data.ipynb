{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and parse data with TensorFlow 2.0 (tf.data)\n",
    "\n",
    "A TensorFlow 2.0 example to build input pipelines for loading data efficiently.\n",
    "\n",
    "\n",
    "- Numpy Arrays\n",
    "- Images\n",
    "- CSV file\n",
    "- Custom data from a Generator\n",
    "\n",
    "For more information about creating and loading TensorFlow's `TFRecords` data format, see: [tfrecords.ipynb](tfrecords.ipynb)\n",
    "\n",
    "- Author: Aymeric Damien\n",
    "- Project: https://github.com/aymericdamien/TensorFlow-Examples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import requests\n",
    "import string\n",
    "import tarfile\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Numpy Arrays\n",
    "\n",
    "Build a data pipeline over numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a toy dataset (even and odd numbers, with respective labels of 0 and 1).\n",
    "evens = np.arange(0, 100, step=2, dtype=np.int32)\n",
    "evens_label = np.zeros(50, dtype=np.int32)\n",
    "odds = np.arange(1, 100, step=2, dtype=np.int32)\n",
    "odds_label = np.ones(50, dtype=np.int32)\n",
    "# Concatenate arrays\n",
    "features = np.concatenate([evens, odds])\n",
    "labels = np.concatenate([evens_label, odds_label])\n",
    "\n",
    "# Load a numpy array using tf data api with `from_tensor_slices`.\n",
    "data = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "# Refill data indefinitely.  \n",
    "data = data.repeat()\n",
    "# Shuffle data.\n",
    "data = data.shuffle(buffer_size=100)\n",
    "# Batch data (aggregate records together).\n",
    "data = data.batch(batch_size=4)\n",
    "# Prefetch batch (pre-load batch for faster consumption).\n",
    "data = data.prefetch(buffer_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([24 81 85 96], shape=(4,), dtype=int32) tf.Tensor([0 1 1 0], shape=(4,), dtype=int32)\n",
      "tf.Tensor([18 37 83 47], shape=(4,), dtype=int32) tf.Tensor([0 1 1 1], shape=(4,), dtype=int32)\n",
      "tf.Tensor([25 58 18  6], shape=(4,), dtype=int32) tf.Tensor([1 0 0 0], shape=(4,), dtype=int32)\n",
      "tf.Tensor([45 32  2 14], shape=(4,), dtype=int32) tf.Tensor([1 0 0 0], shape=(4,), dtype=int32)\n",
      "tf.Tensor([38 30 98 68], shape=(4,), dtype=int32) tf.Tensor([0 0 0 0], shape=(4,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for batch_x, batch_y in data.take(5):\n",
    "    print(batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([72 12  5 64], shape=(4,), dtype=int32) tf.Tensor([0 0 1 0], shape=(4,), dtype=int32)\n",
      "tf.Tensor([ 6  0  2 93], shape=(4,), dtype=int32) tf.Tensor([0 0 0 1], shape=(4,), dtype=int32)\n",
      "tf.Tensor([79 37  6 94], shape=(4,), dtype=int32) tf.Tensor([1 1 0 0], shape=(4,), dtype=int32)\n",
      "tf.Tensor([62 16 17 28], shape=(4,), dtype=int32) tf.Tensor([0 0 1 0], shape=(4,), dtype=int32)\n",
      "tf.Tensor([11 48  0 70], shape=(4,), dtype=int32) tf.Tensor([1 0 0 0], shape=(4,), dtype=int32)\n",
      "tf.Tensor([28 22 20 71], shape=(4,), dtype=int32) tf.Tensor([0 0 0 1], shape=(4,), dtype=int32)\n",
      "tf.Tensor([ 8  1 57 52], shape=(4,), dtype=int32) tf.Tensor([0 1 1 0], shape=(4,), dtype=int32)\n",
      "tf.Tensor([44  7 44  2], shape=(4,), dtype=int32) tf.Tensor([0 1 0 0], shape=(4,), dtype=int32)\n",
      "tf.Tensor([48 34 32  4], shape=(4,), dtype=int32) tf.Tensor([0 0 0 0], shape=(4,), dtype=int32)\n",
      "tf.Tensor([51 95 65 76], shape=(4,), dtype=int32) tf.Tensor([1 1 1 0], shape=(4,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Note: If you are planning on calling multiple time,\n",
    "# you can user the iterator way:\n",
    "ite_data = iter(data)\n",
    "for i in range(5):\n",
    "    batch_x, batch_y = next(ite_data)\n",
    "    print(batch_x, batch_y)\n",
    "\n",
    "for i in range(5):\n",
    "    batch_x, batch_y = next(ite_data)\n",
    "    print(batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CSV files\n",
    "\n",
    "Build a data pipeline from features stored in a CSV file. For this example, Titanic dataset will be used as a toy dataset stored in CSV format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Titanic Dataset\n",
    "\n",
    "\n",
    "\n",
    "survived|pclass|name|sex|age|sibsp|parch|ticket|fare\n",
    "--------|------|----|---|---|-----|-----|------|----\n",
    "1|1|\"Allen, Miss. Elisabeth Walton\"|female|29|0|0|24160|211.3375\n",
    "1|1|\"Allison, Master. Hudson Trevor\"|male|0.9167|1|2|113781|151.5500\n",
    "0|1|\"Allison, Miss. Helen Loraine\"|female|2|1|2|113781|151.5500\n",
    "0|1|\"Allison, Mr. Hudson Joshua Creighton\"|male|30|1|2|113781|151.5500\n",
    "...|...|...|...|...|...|...|...|..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Titanic dataset (in csv format).\n",
    "d = requests.get(\"https://raw.githubusercontent.com/tflearn/tflearn.github.io/master/resources/titanic_dataset.csv\")\n",
    "with open(\"titanic_dataset.csv\", \"wb\") as f:\n",
    "    f.write(d.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Titanic dataset.\n",
    "# Original features: survived,pclass,name,sex,age,sibsp,parch,ticket,fare\n",
    "# Select specific columns: survived,pclass,name,sex,age,fare\n",
    "column_to_use = [0, 1, 2, 3, 4, 8]\n",
    "record_defaults = [tf.int32, tf.int32, tf.string, tf.string, tf.float32, tf.float32]\n",
    "\n",
    "# Load the whole dataset file, and slice each line.\n",
    "data = tf.data.experimental.CsvDataset(\"titanic_dataset.csv\", record_defaults, header=True, select_cols=column_to_use)\n",
    "# Refill data indefinitely.\n",
    "data = data.repeat()\n",
    "# Shuffle data.\n",
    "data = data.shuffle(buffer_size=1000)\n",
    "# Batch data (aggregate records together).\n",
    "data = data.batch(batch_size=2)\n",
    "# Prefetch batch (pre-load batch for faster consumption).\n",
    "data = data.prefetch(buffer_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1]\n",
      "[1 3]\n",
      "[b'Ostby, Miss. Helene Ragnhild' b'Asplund, Master. Edvin Rojj Felix']\n",
      "[b'female' b'male']\n",
      "[22.  3.]\n",
      "[61.9792 31.3875]\n"
     ]
    }
   ],
   "source": [
    "for survived, pclass, name, sex, age, fare in data.take(1):\n",
    "    print(survived.numpy())\n",
    "    print(pclass.numpy())\n",
    "    print(name.numpy())\n",
    "    print(sex.numpy())\n",
    "    print(age.numpy())\n",
    "    print(fare.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Images\n",
    "\n",
    "Build a data pipeline by loading images from disk. For this example, Oxford Flowers dataset will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Oxford 17 flowers dataset\n",
    "d = requests.get(\"http://www.robots.ox.ac.uk/~vgg/data/flowers/17/17flowers.tgz\")\n",
    "with open(\"17flowers.tgz\", \"wb\") as f:\n",
    "    f.write(d.content)\n",
    "# Extract archive.\n",
    "with tarfile.open(\"17flowers.tgz\") as t:\n",
    "    t.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('jpg/dataset.csv', 'w') as f:\n",
    "    c = 0\n",
    "    for i in range(1360):\n",
    "        f.write(\"jpg/image_%04i.jpg,%i\\n\" % (i+1, c))\n",
    "        if (i+1) % 80 == 0:\n",
    "            c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Images\n",
    "with open(\"jpg/dataset.csv\") as f:\n",
    "    dataset_file = f.read().splitlines()\n",
    "\n",
    "# Load the whole dataset file, and slice each line.\n",
    "data = tf.data.Dataset.from_tensor_slices(dataset_file)\n",
    "# Refill data indefinitely.\n",
    "data = data.repeat()\n",
    "# Shuffle data.\n",
    "data = data.shuffle(buffer_size=1000)\n",
    "\n",
    "# Load and pre-process images.\n",
    "def load_image(path):\n",
    "    # Read image from path.\n",
    "    image = tf.io.read_file(path)\n",
    "    # Decode the jpeg image to array [0, 255].\n",
    "    image = tf.image.decode_jpeg(image)\n",
    "    # Resize images to a common size of 256x256.\n",
    "    image = tf.image.resize(image, [256, 256])\n",
    "    # Rescale values to [-1, 1].\n",
    "    image = 1. - image / 127.5\n",
    "    return image\n",
    "# Decode each line from the dataset file.\n",
    "def parse_records(line):\n",
    "    # File is in csv format: \"image_path,label_id\".\n",
    "    # TensorFlow requires a default value, but it will never be used.\n",
    "    image_path, image_label = tf.io.decode_csv(line, [\"\", 0])\n",
    "    # Apply the function to load images.\n",
    "    image = load_image(image_path)\n",
    "    return image, image_label\n",
    "# Use 'map' to apply the above functions in parallel.\n",
    "data = data.map(parse_records, num_parallel_calls=4)\n",
    "\n",
    "# Batch data (aggregate images-array together).\n",
    "data = data.batch(batch_size=2)\n",
    "# Prefetch batch (pre-load batch for faster consumption).\n",
    "data = data.prefetch(buffer_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[-4.83272076e-01  7.79840708e-01  6.97303891e-01]\n",
      "   [-4.34444427e-01  7.91131496e-01  7.23598838e-01]\n",
      "   [-3.99359941e-01  7.99816191e-01  7.40073562e-01]\n",
      "   ...\n",
      "   [-3.79255176e-01  7.44573832e-01  7.55208313e-01]\n",
      "   [-3.82046580e-01  7.39522099e-01  7.70894587e-01]\n",
      "   [-3.96341205e-01  7.25227356e-01  7.56599903e-01]]\n",
      "\n",
      "  [[-4.34366822e-01  7.83341467e-01  7.04113543e-01]\n",
      "   [-3.69448423e-01  8.15980196e-01  7.47757256e-01]\n",
      "   [-3.01225543e-01  8.44281375e-01  7.89583325e-01]\n",
      "   ...\n",
      "   [-2.63991117e-01  8.12275469e-01  8.31985295e-01]\n",
      "   [-3.33664179e-01  7.81948447e-01  7.96691179e-01]\n",
      "   [-3.78308773e-01  7.38725483e-01  7.52757370e-01]]\n",
      "\n",
      "  [[-4.11390305e-01  7.59252429e-01  6.91087902e-01]\n",
      "   [-3.26000929e-01  8.03288221e-01  7.50183821e-01]\n",
      "   [-2.16925621e-01  8.42109442e-01  8.12928915e-01]\n",
      "   ...\n",
      "   [-2.23175645e-01  8.07734489e-01  8.35458279e-01]\n",
      "   [-3.13827991e-01  7.88786769e-01  7.98815191e-01]\n",
      "   [-3.92708302e-01  7.21017122e-01  7.28860259e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-3.42007041e-01  7.57938385e-01  7.32965708e-01]\n",
      "   [-2.75653958e-01  8.07373524e-01  7.73488224e-01]\n",
      "   [-1.92995071e-01  8.43811274e-01  8.22473884e-01]\n",
      "   ...\n",
      "   [-1.85416698e-01  8.31263900e-01  8.36859465e-01]\n",
      "   [-3.06842208e-01  8.09089184e-01  8.08088720e-01]\n",
      "   [-3.72916698e-01  7.56495118e-01  7.56495118e-01]]\n",
      "\n",
      "  [[-3.95510674e-01  7.49587357e-01  7.29366779e-01]\n",
      "   [-3.33590865e-01  7.98578262e-01  7.81727731e-01]\n",
      "   [-2.65910745e-01  8.32822561e-01  8.11580896e-01]\n",
      "   ...\n",
      "   [-2.63603926e-01  8.16052973e-01  8.18340719e-01]\n",
      "   [-3.28603268e-01  7.97377169e-01  7.99092829e-01]\n",
      "   [-3.79893780e-01  7.52826750e-01  7.51172364e-01]]\n",
      "\n",
      "  [[-3.76917243e-01  7.68180847e-01  7.68180847e-01]\n",
      "   [-3.73337507e-01  7.71760583e-01  7.71760583e-01]\n",
      "   [-3.66571426e-01  7.57257700e-01  7.65100837e-01]\n",
      "   ...\n",
      "   [-3.63759518e-01  7.50646710e-01  7.65992641e-01]\n",
      "   [-3.73651981e-01  7.84865201e-01  7.72549033e-01]\n",
      "   [-3.92340660e-01  7.76286781e-01  7.60600507e-01]]]\n",
      "\n",
      "\n",
      " [[[ 4.81882095e-02 -6.94588423e-02  1.10933304e-01]\n",
      "   [ 2.08823502e-01  9.11764503e-02  2.71568656e-01]\n",
      "   [ 2.55759776e-01  1.38112724e-01  3.18504930e-01]\n",
      "   ...\n",
      "   [ 3.61131012e-01  3.07393253e-01  3.81657958e-01]\n",
      "   [ 3.53599906e-01  2.69592524e-01  4.16345000e-01]\n",
      "   [ 3.60340059e-01  2.66222417e-01  4.23085153e-01]]\n",
      "\n",
      "  [[ 1.17389381e-01 -2.57611275e-04  1.80134475e-01]\n",
      "   [ 2.80867040e-01  1.63219988e-01  3.43612134e-01]\n",
      "   [ 3.35171580e-01  2.17524529e-01  3.97916675e-01]\n",
      "   ...\n",
      "   [ 4.17827785e-01  3.62758875e-01  4.45309401e-01]\n",
      "   [ 4.12867665e-01  3.28860283e-01  4.75612760e-01]\n",
      "   [ 4.19607818e-01  3.25490177e-01  4.82352912e-01]]\n",
      "\n",
      "  [[ 1.36351109e-01  1.87040567e-02  1.99096203e-01]\n",
      "   [ 2.82429516e-01  1.64782465e-01  3.45174611e-01]\n",
      "   [ 3.20756733e-01  2.03109682e-01  3.83501828e-01]\n",
      "   ...\n",
      "   [ 3.81880581e-01  3.25140357e-01  4.18093801e-01]\n",
      "   [ 4.12867665e-01  3.28860283e-01  4.75612760e-01]\n",
      "   [ 4.19607818e-01  3.25490177e-01  4.82352912e-01]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 8.36596191e-01  8.68265450e-01  9.44898903e-01]\n",
      "   [ 9.00762796e-01  9.32458639e-01  9.98706996e-01]\n",
      "   [ 8.72398376e-01  9.03770924e-01  9.84420955e-01]\n",
      "   ...\n",
      "   [ 4.56661105e-01  3.39014053e-01  5.72469950e-01]\n",
      "   [ 7.97873616e-01  6.88069701e-01  8.75202000e-01]\n",
      "   [ 8.36782575e-01  7.26978660e-01  9.07370806e-01]]\n",
      "\n",
      "  [[ 8.22977960e-01  8.54350507e-01  9.48468149e-01]\n",
      "   [ 8.56947601e-01  8.88320148e-01  9.82437789e-01]\n",
      "   [ 8.05070460e-01  8.36443007e-01  9.30560648e-01]\n",
      "   ...\n",
      "   [ 2.52317786e-01  1.34670734e-01  3.68126631e-01]\n",
      "   [ 6.72219038e-01  5.62415123e-01  7.49547482e-01]\n",
      "   [ 8.72061610e-01  7.62257636e-01  9.42649841e-01]]\n",
      "\n",
      "  [[ 6.69270873e-01  7.05009222e-01  7.96216309e-01]\n",
      "   [ 6.95983768e-01  7.31722116e-01  8.22929263e-01]\n",
      "   [ 6.50888443e-01  6.86626852e-01  7.77833939e-01]\n",
      "   ...\n",
      "   [ 2.14938760e-01  9.72917080e-02  3.30747604e-01]\n",
      "   [ 4.87689674e-01  3.77885759e-01  5.65018117e-01]\n",
      "   [ 6.30454779e-01  5.20650864e-01  7.01043010e-01]]]], shape=(2, 256, 256, 3), dtype=float32) tf.Tensor([6 2], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for batch_x, batch_y in data.take(1):\n",
    "    print(batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from a Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy generator.\n",
    "def generate_features():\n",
    "    # Function to generate a random string.\n",
    "    def random_string(length):\n",
    "        return ''.join(random.choice(string.ascii_letters) for m in xrange(length))\n",
    "    # Return a random string, a random vector, and a random int.\n",
    "    yield random_string(4), np.random.uniform(size=4), random.randint(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a numpy array using tf data api with `from_tensor_slices`.\n",
    "data = tf.data.Dataset.from_generator(generate_features, output_types=(tf.string, tf.float32, tf.int32))\n",
    "# Refill data indefinitely.\n",
    "data = data.repeat()\n",
    "# Shuffle data.\n",
    "data = data.shuffle(buffer_size=100)\n",
    "# Batch data (aggregate records together).\n",
    "data = data.batch(batch_size=4)\n",
    "# Prefetch batch (pre-load batch for faster consumption).\n",
    "data = data.prefetch(buffer_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'rjlG' b'trSb' b'Odgh' b'QTDL'], shape=(4,), dtype=string) tf.Tensor(\n",
      "[[0.7968671  0.15037513 0.7494175  0.7291446 ]\n",
      " [0.45667616 0.49803782 0.00560279 0.8808681 ]\n",
      " [0.59517807 0.887676   0.6485402  0.3936121 ]\n",
      " [0.64150053 0.6131235  0.4755623  0.49115276]], shape=(4, 4), dtype=float32) tf.Tensor([3 1 8 6], shape=(4,), dtype=int32)\n",
      "tf.Tensor([b'kVJb' b'ulfP' b'GUVm' b'OMjg'], shape=(4,), dtype=string) tf.Tensor(\n",
      "[[0.81523997 0.42112964 0.68366605 0.30692655]\n",
      " [0.8287681  0.44852808 0.23755868 0.5468942 ]\n",
      " [0.6098704  0.21439378 0.6970029  0.84742945]\n",
      " [0.13671751 0.7290514  0.29603773 0.9415665 ]], shape=(4, 4), dtype=float32) tf.Tensor([5 1 5 7], shape=(4,), dtype=int32)\n",
      "tf.Tensor([b'XIEn' b'dZjY' b'xwLI' b'RHUY'], shape=(4,), dtype=string) tf.Tensor(\n",
      "[[0.6253043  0.75604796 0.04800427 0.8301756 ]\n",
      " [0.07147692 0.9510518  0.81060547 0.6188102 ]\n",
      " [0.35208228 0.52762914 0.9284128  0.76412773]\n",
      " [0.09099609 0.7309415  0.55512774 0.30464917]], shape=(4, 4), dtype=float32) tf.Tensor([5 0 6 8], shape=(4,), dtype=int32)\n",
      "tf.Tensor([b'SLCF' b'fyQS' b'Sung' b'WEHh'], shape=(4,), dtype=string) tf.Tensor(\n",
      "[[0.8633745  0.14131083 0.06216403 0.329325  ]\n",
      " [0.9790663  0.34664872 0.5220373  0.3629093 ]\n",
      " [0.30946922 0.08947354 0.87141734 0.88593245]\n",
      " [0.03810362 0.49999726 0.3184147  0.8463561 ]], shape=(4, 4), dtype=float32) tf.Tensor([5 6 5 2], shape=(4,), dtype=int32)\n",
      "tf.Tensor([b'bNJf' b'NWES' b'eOIK' b'zcQc'], shape=(4,), dtype=string) tf.Tensor(\n",
      "[[0.16605608 0.5386418  0.8978918  0.7970079 ]\n",
      " [0.16889176 0.623404   0.09693018 0.47654706]\n",
      " [0.33496565 0.7850415  0.60069346 0.85061824]\n",
      " [0.4927015  0.18043903 0.25607252 0.5218471 ]], shape=(4, 4), dtype=float32) tf.Tensor([7 0 9 3], shape=(4,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Display data.\n",
    "xrange = range\n",
    "for batch_str, batch_vector, batch_int in data.take(5):\n",
    "    print(batch_str, batch_vector, batch_int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
